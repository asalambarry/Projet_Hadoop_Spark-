{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8f53-dfce-424a-b5b1-1d7c3511701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92218560-dffb-445f-ba8f-79c1d68c3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = KafkaConsumer('movielens_ratings' , bootstrap_servers=['localhost:9092'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9937ad5-207a-40ff-9620-a3b40c4f8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_msg(msg):\n",
    "    print(msg.offset)\n",
    "    dico = dict(json.loads(msg.value))\n",
    "    print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ff0af-c6ba-4671-9be9-0e6e23888cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in c:\n",
    "    process_msg(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0fd42-40f8-496f-a114-0ba438b90a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'MoviesRatings',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Pour lire depuis le début\n",
    "    group_id='my-group',  # Identifiant de groupe de consommateurs\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))  # Pour désérialiser automatiquement\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    print(f\"Offset: {message.offset}\")\n",
    "    print(f\"Valeur: {message.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51bd60-09cb-482c-ab4d-a04aa831f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423c71a-77ec-4690-8f8c-104279200336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e0234-ff5e-493a-873f-adf26a0352e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_recommendations.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, array, lit, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "# Créer une session Spark avec support Kafka et MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecommendationStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le schéma des données entrantes\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Charger le modèle ALS préalablement entraîné\n",
    "model_path = \"hdfs://namenode:9000/models/als\"\n",
    "model = ALSModel.load(model_path)\n",
    "\n",
    "\n",
    "# Configuration MongoDB\n",
    "mongo_uri = \"mongodb://localhost:27017\"\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "mongo_db = mongo_client[\"movie_recommender\"]\n",
    "mongo_collection = mongo_db[\"recommendations\"]\n",
    "\n",
    "# Fonction pour générer des recommandations et les sauvegarder dans MongoDB\n",
    "def process_batch(df, epoch_id):\n",
    "    if not df.isEmpty():\n",
    "        try:\n",
    "            # Extraire les userId uniques du batch\n",
    "            unique_users = df.select(\"userId\").distinct()\n",
    "            \n",
    "            # Générer des recommandations pour chaque utilisateur\n",
    "            recommendations = model.recommendForUserSubset(unique_users, 10)\n",
    "            \n",
    "            # Exploser les recommandations pour avoir un format plat\n",
    "            flat_recommendations = recommendations.select(\n",
    "                col(\"userId\"),\n",
    "                explode(col(\"recommendations\")).alias(\"rec\")\n",
    "            ).select(\n",
    "                col(\"userId\"),\n",
    "                col(\"rec.movieId\").alias(\"movieId\"),\n",
    "                col(\"rec.rating\").alias(\"prediction\")\n",
    "            )\n",
    "            \n",
    "            # Joindre avec les informations de films (si disponible)\n",
    "            try:\n",
    "                movies_df = spark.read.csv(\"hdfs://namenode:9000/datasets/movies.csv\", header=True)\n",
    "                recommendations_with_info = flat_recommendations.join(\n",
    "                    movies_df, flat_recommendations.movieId == movies_df.movieId\n",
    "                ).select(\n",
    "                    flat_recommendations.userId,\n",
    "                    flat_recommendations.movieId,\n",
    "                    movies_df.title,\n",
    "                    flat_recommendations.prediction\n",
    "                )\n",
    "            except:\n",
    "                recommendations_with_info = flat_recommendations\n",
    "            \n",
    "            # Afficher les recommandations\n",
    "            recommendations_with_info.show(10, False)\n",
    "            \n",
    "            # Convertir le DataFrame en liste de dictionnaires pour MongoDB\n",
    "            recommendations_list = recommendations_with_info.withColumn(\n",
    "                \"timestamp\", lit(datetime.datetime.now().isoformat())\n",
    "            ).toJSON().collect()\n",
    "            \n",
    "            # Insérer dans MongoDB\n",
    "            if recommendations_list:\n",
    "                documents = [json.loads(rec) for rec in recommendations_list]\n",
    "                mongo_collection.insert_many(documents)\n",
    "                \n",
    "            print(f\"Batch {epoch_id}: Recommandations générées et sauvegardées pour {unique_users.count()} utilisateurs\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du batch {epoch_id}: {str(e)}\")\n",
    "\n",
    "# Lire les données du stream Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"MoviesRatings\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformer les données JSON en DataFrame structuré\n",
    "parsed_stream = kafka_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Traiter les données par batch\n",
    "query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Attendre la fin du traitement\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb109d-6f17-4844-be52-2df714c109e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-88488a41-288b-4028-b6ae-e1d8be3b380d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 1112ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-88488a41-288b-4028-b6ae-e1d8be3b380d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/10ms)\n",
      "25/05/02 15:24:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/02 15:24:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2a26359a-a8b4-4b7c-97ae-3ac1cfed360b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/02 15:24:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/02 15:25:00 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "25/05/02 15:25:00 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "25/05/02 15:25:00 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "25/05/02 15:25:00 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "25/05/02 15:25:00 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:25:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|3     |120821 |6.3276896 |\n",
      "|3     |117907 |5.928648  |\n",
      "|3     |98595  |5.924997  |\n",
      "|3     |101538 |5.8830857 |\n",
      "|3     |125966 |5.779213  |\n",
      "|3     |102119 |5.7608495 |\n",
      "|3     |74061  |5.738727  |\n",
      "|3     |120313 |5.7309647 |\n",
      "|3     |129514 |5.7032986 |\n",
      "|3     |109887 |5.682914  |\n",
      "+------+-------+----------+\n",
      "\n",
      "Erreur lors du traitement du batch 1: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:25:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 37441 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|85    |120821 |5.961138  |\n",
      "|85    |98595  |5.7224665 |\n",
      "|85    |86237  |5.562018  |\n",
      "|85    |77736  |5.491927  |\n",
      "|85    |101855 |5.4835405 |\n",
      "|85    |60356  |5.479856  |\n",
      "|85    |85205  |5.4434586 |\n",
      "|85    |117907 |5.338574  |\n",
      "|85    |74061  |5.305612  |\n",
      "|85    |106334 |5.2770243 |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Erreur lors du traitement du batch 2: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:26:13 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 35009 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|85    |120821 |5.961138  |\n",
      "|85    |98595  |5.7224665 |\n",
      "|85    |86237  |5.562018  |\n",
      "|85    |77736  |5.491927  |\n",
      "|85    |101855 |5.4835405 |\n",
      "|85    |60356  |5.479856  |\n",
      "|85    |85205  |5.4434586 |\n",
      "|85    |117907 |5.338574  |\n",
      "|85    |74061  |5.305612  |\n",
      "|85    |106334 |5.2770243 |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Erreur lors du traitement du batch 3: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:26:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 34408 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|52    |120821 |5.8714366 |\n",
      "|52    |7568   |4.966122  |\n",
      "|52    |87948  |4.6657233 |\n",
      "|52    |83531  |4.642039  |\n",
      "|52    |26459  |4.5690837 |\n",
      "|52    |6085   |4.53474   |\n",
      "|52    |103838 |4.5006123 |\n",
      "|52    |47460  |4.4819455 |\n",
      "|52    |3582   |4.469969  |\n",
      "|52    |77736  |4.413271  |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Erreur lors du traitement du batch 4: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:27:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 34102 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|5     |120821 |6.0885944 |\n",
      "|5     |77736  |5.9700994 |\n",
      "|5     |101855 |5.8657312 |\n",
      "|5     |86237  |5.821442  |\n",
      "|5     |85205  |5.6868367 |\n",
      "|5     |41980  |5.6113367 |\n",
      "|5     |98595  |5.5689597 |\n",
      "|5     |129243 |5.5292807 |\n",
      "|5     |120313 |5.505905  |\n",
      "|5     |83531  |5.4922004 |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Erreur lors du traitement du batch 5: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:27:56 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 33880 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|12    |120821 |5.3279204 |\n",
      "|12    |117907 |5.152245  |\n",
      "|12    |98595  |5.051058  |\n",
      "|12    |101538 |4.9056544 |\n",
      "|12    |101855 |4.812896  |\n",
      "|12    |112423 |4.754116  |\n",
      "|12    |104103 |4.717584  |\n",
      "|12    |102107 |4.717584  |\n",
      "|12    |32230  |4.709983  |\n",
      "|12    |74061  |4.683931  |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Erreur lors du traitement du batch 6: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6814e3cb6b5defe400436fd8, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 15:28:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 33838 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des informations des films: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/datasets/movies.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|85    |120821 |5.961138  |\n",
      "|85    |98595  |5.7224665 |\n",
      "|85    |86237  |5.562018  |\n",
      "|85    |77736  |5.491927  |\n",
      "|85    |101855 |5.4835405 |\n",
      "|85    |60356  |5.479856  |\n",
      "|85    |85205  |5.4434586 |\n",
      "|85    |117907 |5.338574  |\n",
      "|85    |74061  |5.305612  |\n",
      "|85    |106334 |5.2770243 |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, lit\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Créer une session Spark avec support Kafka et MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecommendationStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le schéma des données entrantes\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Charger le modèle ALS préalablement entraîné\n",
    "model_path = \"hdfs://namenode:9000/models/als\"\n",
    "model = ALSModel.load(model_path)\n",
    "\n",
    "# Configuration MongoDB\n",
    "mongo_uri = \"mongodb://localhost:27017\"\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "mongo_db = mongo_client[\"movie_recommender\"]\n",
    "mongo_collection = mongo_db[\"recommendations\"]\n",
    "\n",
    "# Fonction pour générer des recommandations et les sauvegarder dans MongoDB\n",
    "def process_batch(df, epoch_id):\n",
    "    if not df.isEmpty():\n",
    "        try:\n",
    "            # Extraire les userId uniques du batch\n",
    "            unique_users = df.select(\"userId\").distinct()\n",
    "            \n",
    "            # Générer des recommandations pour chaque utilisateur\n",
    "            recommendations = model.recommendForUserSubset(unique_users, 10)\n",
    "            \n",
    "            # Exploser les recommandations pour avoir un format plat\n",
    "            flat_recommendations = recommendations.select(\n",
    "                col(\"userId\"),\n",
    "                explode(col(\"recommendations\")).alias(\"rec\")\n",
    "            ).select(\n",
    "                col(\"userId\"),\n",
    "                col(\"rec.movieId\").alias(\"movieId\"),\n",
    "                col(\"rec.rating\").alias(\"prediction\")\n",
    "            )\n",
    "            \n",
    "            # Joindre avec les informations de films (si disponible)\n",
    "            try:\n",
    "                movies_df = spark.read.csv(\"hdfs://namenode:9000/datasets/movie.csv\", header=True)\n",
    "                recommendations_with_info = flat_recommendations.join(\n",
    "                    movies_df, flat_recommendations.movieId == movies_df.movieId\n",
    "                ).select(\n",
    "                    flat_recommendations.userId,\n",
    "                    flat_recommendations.movieId,\n",
    "                    movies_df.title,\n",
    "                    flat_recommendations.prediction\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement des informations des films: {str(e)}\")\n",
    "                recommendations_with_info = flat_recommendations\n",
    "            \n",
    "            # Afficher les recommandations (facultatif, pour debug)\n",
    "            recommendations_with_info.show(10, False)\n",
    "            \n",
    "            # Convertir le DataFrame en liste de dictionnaires pour MongoDB\n",
    "            recommendations_list = recommendations_with_info.withColumn(\n",
    "                \"timestamp\", lit(datetime.datetime.now().isoformat())\n",
    "            ).toJSON().collect()\n",
    "            \n",
    "            # Insérer dans MongoDB\n",
    "            if recommendations_list:\n",
    "                documents = [json.loads(rec) for rec in recommendations_list]\n",
    "                mongo_collection.insert_many(documents)\n",
    "                \n",
    "            print(f\"Batch {epoch_id}: Recommandations générées et sauvegardées pour {unique_users.count()} utilisateurs\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du batch {epoch_id}: {str(e)}\")\n",
    "\n",
    "# Lire les données du stream Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"MoviesRatings\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformer les données JSON en DataFrame structuré\n",
    "parsed_stream = kafka_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Traiter les données par batch\n",
    "query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Attendre la fin du traitement\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c56c5-a392-4d2a-82d4-9cb9a86ef349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
