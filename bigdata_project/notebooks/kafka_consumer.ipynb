{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8f53-dfce-424a-b5b1-1d7c3511701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92218560-dffb-445f-ba8f-79c1d68c3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = KafkaConsumer('movielens_ratings' , bootstrap_servers=['localhost:9092'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9937ad5-207a-40ff-9620-a3b40c4f8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_msg(msg):\n",
    "    print(msg.offset)\n",
    "    dico = dict(json.loads(msg.value))\n",
    "    print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ff0af-c6ba-4671-9be9-0e6e23888cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in c:\n",
    "    process_msg(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0fd42-40f8-496f-a114-0ba438b90a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'MoviesRatings',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Pour lire depuis le début\n",
    "    group_id='my-group',  # Identifiant de groupe de consommateurs\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))  # Pour désérialiser automatiquement\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    print(f\"Offset: {message.offset}\")\n",
    "    print(f\"Valeur: {message.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51bd60-09cb-482c-ab4d-a04aa831f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423c71a-77ec-4690-8f8c-104279200336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e0234-ff5e-493a-873f-adf26a0352e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_recommendations.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, array, lit, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "# Créer une session Spark avec support Kafka et MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecommendationStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le schéma des données entrantes\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Charger le modèle ALS préalablement entraîné\n",
    "model_path = \"hdfs://namenode:9000/models/als\"\n",
    "model = ALSModel.load(model_path)\n",
    "\n",
    "\n",
    "# Configuration MongoDB\n",
    "mongo_uri = \"mongodb://localhost:27017\"\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "mongo_db = mongo_client[\"movie_recommender\"]\n",
    "mongo_collection = mongo_db[\"recommendations\"]\n",
    "\n",
    "# Fonction pour générer des recommandations et les sauvegarder dans MongoDB\n",
    "def process_batch(df, epoch_id):\n",
    "    if not df.isEmpty():\n",
    "        try:\n",
    "            # Extraire les userId uniques du batch\n",
    "            unique_users = df.select(\"userId\").distinct()\n",
    "            \n",
    "            # Générer des recommandations pour chaque utilisateur\n",
    "            recommendations = model.recommendForUserSubset(unique_users, 10)\n",
    "            \n",
    "            # Exploser les recommandations pour avoir un format plat\n",
    "            flat_recommendations = recommendations.select(\n",
    "                col(\"userId\"),\n",
    "                explode(col(\"recommendations\")).alias(\"rec\")\n",
    "            ).select(\n",
    "                col(\"userId\"),\n",
    "                col(\"rec.movieId\").alias(\"movieId\"),\n",
    "                col(\"rec.rating\").alias(\"prediction\")\n",
    "            )\n",
    "            \n",
    "            # Joindre avec les informations de films (si disponible)\n",
    "            try:\n",
    "                movies_df = spark.read.csv(\"hdfs://namenode:9000/datasets/movies.csv\", header=True)\n",
    "                recommendations_with_info = flat_recommendations.join(\n",
    "                    movies_df, flat_recommendations.movieId == movies_df.movieId\n",
    "                ).select(\n",
    "                    flat_recommendations.userId,\n",
    "                    flat_recommendations.movieId,\n",
    "                    movies_df.title,\n",
    "                    flat_recommendations.prediction\n",
    "                )\n",
    "            except:\n",
    "                recommendations_with_info = flat_recommendations\n",
    "            \n",
    "            # Afficher les recommandations\n",
    "            recommendations_with_info.show(10, False)\n",
    "            \n",
    "            # Convertir le DataFrame en liste de dictionnaires pour MongoDB\n",
    "            recommendations_list = recommendations_with_info.withColumn(\n",
    "                \"timestamp\", lit(datetime.datetime.now().isoformat())\n",
    "            ).toJSON().collect()\n",
    "            \n",
    "            # Insérer dans MongoDB\n",
    "            if recommendations_list:\n",
    "                documents = [json.loads(rec) for rec in recommendations_list]\n",
    "                mongo_collection.insert_many(documents)\n",
    "                \n",
    "            print(f\"Batch {epoch_id}: Recommandations générées et sauvegardées pour {unique_users.count()} utilisateurs\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du batch {epoch_id}: {str(e)}\")\n",
    "\n",
    "# Lire les données du stream Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"MoviesRatings\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformer les données JSON en DataFrame structuré\n",
    "parsed_stream = kafka_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Traiter les données par batch\n",
    "query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Attendre la fin du traitement\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fb109d-6f17-4844-be52-2df714c109e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion Kafka: localhost:9092\n",
      "Connexion MongoDB: mongodb://localhost:27017\n",
      "Connexion HDFS: namenode:9000\n",
      "Tentative de chargement du modèle depuis: hdfs://namenode:9000/models/als\n",
      "Modèle chargé avec succès!\n",
      "Tentative de connexion à MongoDB: mongodb://localhost:27017\n",
      "Erreur de connexion à MongoDB: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 6817d61b07dd3e759ae8be8b, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n",
      "Le programme continuera sans MongoDB - les recommandations ne seront pas sauvegardées.\n",
      "Test de connexion à Kafka sur localhost:9092...\n",
      "Port 9092 sur localhost est ouvert (socket test).\n",
      "Tentative de démarrage du stream Kafka même si le test de connexion échoue...\n",
      "Initialisation du stream Kafka depuis localhost:9092...\n",
      "Stream Kafka initialisé avec succès!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 21:03:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/04 21:03:29 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "25/05/04 21:03:29 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "25/05/04 21:03:29 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "25/05/04 21:03:29 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "25/05/04 21:03:29 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream de traitement démarré! En attente de données...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 21:03:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/04 21:03:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 14503 milliseconds\n",
      "25/05/04 21:03:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10022 milliseconds\n",
      "25/05/04 21:04:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10758 milliseconds\n",
      "25/05/04 21:04:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10032 milliseconds\n",
      "25/05/04 21:05:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10350 milliseconds\n",
      "25/05/04 21:06:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10662 milliseconds\n",
      "25/05/04 21:06:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10127 milliseconds\n",
      "25/05/04 21:07:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10965 milliseconds\n",
      "25/05/04 21:07:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10048 milliseconds\n",
      "25/05/04 21:08:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10116 milliseconds\n",
      "25/05/04 21:11:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10168 milliseconds\n",
      "25/05/04 21:11:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10090 milliseconds\n",
      "25/05/04 21:12:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10402 milliseconds\n",
      "25/05/04 21:12:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10714 milliseconds\n",
      "25/05/04 21:13:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10537 milliseconds\n",
      "25/05/04 21:13:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10404 milliseconds\n",
      "25/05/04 21:15:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10441 milliseconds\n",
      "25/05/04 21:15:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10312 milliseconds\n",
      "25/05/04 21:16:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10632 milliseconds\n",
      "25/05/04 21:17:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10384 milliseconds\n",
      "25/05/04 21:17:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10789 milliseconds\n",
      "25/05/04 21:19:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11925 milliseconds\n",
      "25/05/04 21:20:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10319 milliseconds\n",
      "25/05/04 22:03:27 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 2577118 milliseconds\n",
      "25/05/04 22:04:41 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10834 milliseconds\n",
      "25/05/04 22:05:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10555 milliseconds\n",
      "25/05/04 22:06:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10068 milliseconds\n",
      "25/05/04 22:07:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10317 milliseconds\n",
      "25/05/04 22:07:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10170 milliseconds\n",
      "25/05/04 22:11:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10065 milliseconds\n",
      "25/05/04 22:11:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10428 milliseconds\n",
      "25/05/04 22:12:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10803 milliseconds\n",
      "25/05/04 22:12:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10683 milliseconds\n",
      "25/05/04 22:13:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10619 milliseconds\n",
      "25/05/04 22:15:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10336 milliseconds\n",
      "25/05/04 22:16:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10134 milliseconds\n",
      "25/05/04 22:17:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10115 milliseconds\n",
      "25/05/04 22:18:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10300 milliseconds\n",
      "25/05/04 23:17:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 3385419 milliseconds\n",
      "25/05/05 00:26:42 ERROR Executor: Exception in task 0.0 in stage 6665.0 (TID 55337)\n",
      "org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "25/05/05 00:26:42 WARN TaskSetManager: Lost task 0.0 in stage 6665.0 (TID 55337) (namenode executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "\n",
      "25/05/05 00:26:42 ERROR TaskSetManager: Task 0 in stage 6665.0 failed 1 times; aborting job\n",
      "25/05/05 00:26:43 ERROR MicroBatchExecution: Query [id = 1ce21465-9b19-42c3-80c7-2eeb15d62be4, runId = b4f1793c-f9e7-4def-88dd-0b863fced500] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_48348/2072420800.py\", line 74, in process_batch\n",
      "    if df.isEmpty():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\", line 883, in isEmpty\n",
      "    return self._jdf.isEmpty()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o10337.isEmpty.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6665.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6665.0 (TID 55337) (namenode executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:654)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:653)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:653)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du démarrage du stream: [STREAM_FAILED] Query [id = 1ce21465-9b19-42c3-80c7-2eeb15d62be4, runId = b4f1793c-f9e7-4def-88dd-0b863fced500] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_48348/2072420800.py\", line 74, in process_batch\n",
      "    if df.isEmpty():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\", line 883, in isEmpty\n",
      "    return self._jdf.isEmpty()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o10337.isEmpty.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6665.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6665.0 (TID 55337) (namenode executor driver): org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:654)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:653)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:653)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy44.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition MoviesRatings-0 could be determined\n",
      "\n",
      "\n",
      "Si l'erreur persiste, essayez d'exécuter ce script avec une adresse Kafka différente:\n",
      "KAFKA_SERVER=host.docker.internal:9092 python MovieRecommendationStreaming.py\n",
      "ou trouvez l'adresse IP du broker Kafka et utilisez:\n",
      "KAFKA_SERVER=172.17.0.x:9092 python MovieRecommendationStreaming.py\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, lit\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "import os\n",
    "import socket\n",
    "\n",
    "\n",
    "# Créer une session Spark avec support Kafka et MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecommendationStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le schéma des données entrantes\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Configuration des services - Permettre de spécifier via variables d'environnement ou arguments\n",
    "# Pour MongoDB, nous utilisons localhost comme demandé\n",
    "KAFKA_SERVER = os.environ.get(\"KAFKA_SERVER\", \"localhost:9092\")  # Changé de kafka:9092 à localhost:9092\n",
    "MONGO_URI = \"mongodb://localhost:27017\"  # Fixé à localhost comme demandé\n",
    "HDFS_HOST = os.environ.get(\"HDFS_HOST\", \"namenode:9000\")\n",
    "\n",
    "# Afficher les configurations pour débogage\n",
    "print(f\"Connexion Kafka: {KAFKA_SERVER}\")\n",
    "print(f\"Connexion MongoDB: {MONGO_URI}\")\n",
    "print(f\"Connexion HDFS: {HDFS_HOST}\")\n",
    "\n",
    "try:\n",
    "    # Charger le modèle ALS préalablement entraîné\n",
    "    model_path = f\"hdfs://{HDFS_HOST}/models/als\"\n",
    "    print(f\"Tentative de chargement du modèle depuis: {model_path}\")\n",
    "    model = ALSModel.load(model_path)\n",
    "    print(\"Modèle chargé avec succès!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du modèle: {str(e)}\")\n",
    "    # On continue même si le modèle n'est pas chargé - nous gérerons ce cas dans process_batch\n",
    "\n",
    "# Configuration MongoDB\n",
    "try:\n",
    "    print(f\"Tentative de connexion à MongoDB: {MONGO_URI}\")\n",
    "    mongo_client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n",
    "    # Vérifier la connexion\n",
    "    mongo_client.server_info()\n",
    "    print(\"Connexion à MongoDB établie avec succès!\")\n",
    "    \n",
    "    mongo_db = mongo_client[\"movie_recommender\"]\n",
    "    mongo_collection = mongo_db[\"recommendations\"]\n",
    "except Exception as e:\n",
    "    print(f\"Erreur de connexion à MongoDB: {str(e)}\")\n",
    "    print(\"Le programme continuera sans MongoDB - les recommandations ne seront pas sauvegardées.\")\n",
    "\n",
    "# Fonction pour vérifier la connexion MongoDB\n",
    "def check_mongo_connection():\n",
    "    try:\n",
    "        # Vérifier la connexion à MongoDB\n",
    "        mongo_client.server_info()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur de connexion à MongoDB: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Fonction pour générer des recommandations et les sauvegarder dans MongoDB\n",
    "def process_batch(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        print(f\"Batch {epoch_id}: Aucune donnée reçue.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        print(f\"Traitement du batch {epoch_id}. Nombre d'enregistrements: {df.count()}\")\n",
    "        \n",
    "        # Extraire les userId uniques du batch\n",
    "        unique_users = df.select(\"userId\").distinct()\n",
    "        unique_count = unique_users.count()\n",
    "        print(f\"Nombre d'utilisateurs uniques dans ce batch: {unique_count}\")\n",
    "        \n",
    "        if unique_count == 0:\n",
    "            print(\"Aucun utilisateur à traiter dans ce batch.\")\n",
    "            return\n",
    "        \n",
    "        # Générer des recommandations pour chaque utilisateur\n",
    "        try:\n",
    "            recommendations = model.recommendForUserSubset(unique_users, 10)\n",
    "            print(f\"Recommandations générées pour {recommendations.count()} utilisateurs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la génération des recommandations: {str(e)}\")\n",
    "            return\n",
    "        \n",
    "        # Exploser les recommandations pour avoir un format plat\n",
    "        flat_recommendations = recommendations.select(\n",
    "            col(\"userId\"),\n",
    "            explode(col(\"recommendations\")).alias(\"rec\")\n",
    "        ).select(\n",
    "            col(\"userId\"),\n",
    "            col(\"rec.movieId\").alias(\"movieId\"),\n",
    "            col(\"rec.rating\").alias(\"prediction\")\n",
    "        )\n",
    "        \n",
    "        # Joindre avec les informations de films (si disponible)\n",
    "        try:\n",
    "            movies_path = f\"hdfs://{HDFS_HOST}/datasets/movie.csv\"\n",
    "            print(f\"Tentative de chargement des informations des films depuis: {movies_path}\")\n",
    "            movies_df = spark.read.csv(movies_path, header=True)\n",
    "            recommendations_with_info = flat_recommendations.join(\n",
    "                movies_df, flat_recommendations.movieId == movies_df.movieId\n",
    "            ).select(\n",
    "                flat_recommendations.userId,\n",
    "                flat_recommendations.movieId,\n",
    "                movies_df.title,\n",
    "                flat_recommendations.prediction\n",
    "            )\n",
    "            print(\"Informations des films chargées et jointes avec succès\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement des informations des films: {str(e)}\")\n",
    "            print(\"Continuation sans les informations des films\")\n",
    "            recommendations_with_info = flat_recommendations\n",
    "        \n",
    "        # Afficher les recommandations pour débogage\n",
    "        print(\"Exemple de recommandations générées:\")\n",
    "        recommendations_with_info.show(5, False)\n",
    "        \n",
    "        # Sauvegarder dans MongoDB si disponible\n",
    "        if check_mongo_connection():\n",
    "            try:\n",
    "                # Convertir le DataFrame en liste de dictionnaires pour MongoDB\n",
    "                recommendations_list = recommendations_with_info.withColumn(\n",
    "                    \"timestamp\", lit(datetime.datetime.now().isoformat())\n",
    "                ).toJSON().collect()\n",
    "                \n",
    "                # Insérer dans MongoDB\n",
    "                if recommendations_list:\n",
    "                    documents = [json.loads(rec) for rec in recommendations_list]\n",
    "                    mongo_collection.insert_many(documents)\n",
    "                    print(f\"Données sauvegardées dans MongoDB pour {len(documents)} recommandations\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la sauvegarde dans MongoDB: {str(e)}\")\n",
    "        else:\n",
    "            print(\"MongoDB non disponible, les recommandations ne sont pas sauvegardées\")\n",
    "        \n",
    "        print(f\"Traitement du batch {epoch_id} terminé avec succès!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale lors du traitement du batch {epoch_id}: {str(e)}\")\n",
    "\n",
    "# Ajouter une fonction pour tester la connexion à Kafka\n",
    "def test_kafka_connection():\n",
    "    try:\n",
    "        print(f\"Test de connexion à Kafka sur {KAFKA_SERVER}...\")\n",
    "        # Essayer un test basique de connexion au socket\n",
    "        host, port = KAFKA_SERVER.split(':')\n",
    "        socket_test = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        socket_test.settimeout(5)\n",
    "        result = socket_test.connect_ex((host, int(port)))\n",
    "        socket_test.close()\n",
    "        \n",
    "        if result == 0:\n",
    "            print(f\"Port {port} sur {host} est ouvert (socket test).\")\n",
    "            # Le test socket est réussi, mais continuons quand même\n",
    "            \n",
    "        # On continue sans tester avec KafkaAdminClient pour éviter les erreurs\n",
    "        # Nous verrons si la connexion fonctionne lors de la création du stream\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du test de connexion à Kafka (socket): {str(e)}\")\n",
    "        # On continue quand même - le problème peut être spécifique au test\n",
    "        return True\n",
    "\n",
    "# Tester la connexion à Kafka avant de démarrer le stream\n",
    "test_kafka_connection()\n",
    "print(\"Tentative de démarrage du stream Kafka même si le test de connexion échoue...\")\n",
    "\n",
    "# Lire les données du stream Kafka avec plus d'options de configuration\n",
    "try:\n",
    "    print(f\"Initialisation du stream Kafka depuis {KAFKA_SERVER}...\")\n",
    "    kafka_stream = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_SERVER) \\\n",
    "        .option(\"subscribe\", \"MoviesRatings\") \\\n",
    "        .option(\"startingOffsets\", \"latest\") \\\n",
    "        .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "        .option(\"failOnDataLoss\", \"false\") \\\n",
    "        .option(\"kafkaConsumer.pollTimeoutMs\", \"5000\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"Stream Kafka initialisé avec succès!\")\n",
    "    \n",
    "    # Transformer les données JSON en DataFrame structuré\n",
    "    parsed_stream = kafka_stream \\\n",
    "        .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "    \n",
    "    # Traiter les données par batch avec plus de robustesse\n",
    "    query = parsed_stream \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    print(\"Stream de traitement démarré! En attente de données...\")\n",
    "    \n",
    "    # Attendre la fin du traitement avec gestion des exceptions\n",
    "    query.awaitTermination()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du démarrage du stream: {str(e)}\")\n",
    "    print(\"Si l'erreur persiste, essayez d'exécuter ce script avec une adresse Kafka différente:\")\n",
    "    print(\"KAFKA_SERVER=host.docker.internal:9092 python MovieRecommendationStreaming.py\")\n",
    "    print(\"ou trouvez l'adresse IP du broker Kafka et utilisez:\")\n",
    "    print(\"KAFKA_SERVER=172.17.0.x:9092 python MovieRecommendationStreaming.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c56c5-a392-4d2a-82d4-9cb9a86ef349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d5bff-f582-447a-a2a6-7c45ebfc6502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
